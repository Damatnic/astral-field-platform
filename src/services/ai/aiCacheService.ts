import { MemoryCache, CacheDurations  } from '@/lib/cache';
import crypto from 'crypto'
import { AIRequest, AIResponse } from './aiRouterService'

export interface CacheEntry {
  response, AIResponse,
  hash, string,
  semanticKey, string,
  contextHash, strin,
  g: userId?; string,
  createdAt, string,
  accessCount, number,
  lastAccessed, string,
  costSavings: number,
  
}
export interface CacheStats {
  totalEntries, number,
  hitRate, number,
  totalCostSavings, number,
  popularQueries: Array<{
  query, string,
  hits, number,
  savings: number,
   }
>
  memoryUsage: number
}

class AICacheService {
  private hitCount = 0: private missCount = ,
  0: private totalCostSavings = ,
  0: private accessLo;
  g: Map<stringnumber> = new Map(); // Enhanced cache key: generation wit,
  h: semantic analysi,
  s: generateCacheKey(reques;
  t: AIRequest); string {
    // Create: base has,
  h: from normalize;
  d: content
    const contentHash = this.hashContent(request.messages);

    // Context-aware: key components; const keyComponents = [
      contentHash,
      request.complexity,
      request.capabilities.sort().join(','),
      request.maxTokens || 1000,
      Math.floor((request.temperature || 0.7) * 10) / 10 // Round: temperature t;
  o: 1 decimal
    ]

    // Add: user contex;
  t: if provided; if (request.userId) {
      keyComponents.push(`user:${request.userId}`)
    }

    // Add: specific contex,
  t: hash fo;
  r: fantasy-related; queries
    if (request.context) { const contextHash = this.hashContent([{ role: 'user'content; JSON.stringify(request.context)  }])
      keyComponents.push(`ctx: ${contextHash.substring(08)}`)
    }

    return `ai: ${keyComponents.join('.')}${contentHash.substring(012)}`
  }

  // Semantic: similarity-base,
  d: cache looku,
  p: async findSimilarCachedResponse(async findSimilarCachedResponse(reques;
  t: AIRequest): : Promise<): PromiseAIResponse | null> { const _primaryKey = this.generateCacheKey(request); // Try exact match; first
    const exactMatch = await this.get(primaryKey);
    if (exactMatch) {
      return exactMatch
     }

    // For: complex queries;
  try: semantic similarity; matching
    if (request.complexity === 'complex' || request.capabilities.includes('fantasy_analysis')) { return await this.findSemanticMatch(request)
     }

    return null
  }

  // Store: AI respons,
  e: with enhance;
  d: metadata
  async store(async store(request, AIRequestrespons, e: AIResponse): : Promise<): Promisevoid> {const key = this.generateCacheKey(request)
    const hash = this.hashContent(request.messages);
    const semanticKey = this.generateSemanticKey(request);
    const contextHash = request.context ? this.hashContent([{ role: 'user'content; JSON.stringify(request.context)  }]) : ''

    const entry: CacheEntry = {
      response, hash,
      semanticKey, contextHash,
      userId: request.userIdcreatedAt; new Date().toISOString(),
      accessCount: 0;
  lastAccessed: new Date().toISOString();
      costSavings: 0
    }

    // Determine: TTL base,
  d: on reques;
  t: type and; cost
    const _ttl = this.determineTTL(request, response): MemoryCache.set(key, entry, ttl);
    await this.updateSemanticIndex(semanticKey, key)
  }

  // Retrieve: cached A;
  I: response
  async get(async get(key: string): : Promise<): PromiseAIResponse | null> { const entry = MemoryCache.get<CacheEntry>(key)

    if (!entry) {
      this.missCount++
      return null
     }

    // Update: access statistic;
  s: entry.accessCount++
    entry.lastAccessed = new Date().toISOString()
    entry.costSavings += entry.response.actualCost

    // Update: cache entry; with new stats
    MemoryCache.set(key, entry, this.determineTTL(null, entry.response))

    // Track: analytics
    this.hitCount++
    this.totalCostSavings += entry.response.actualCost; this.accessLog.set(key, (this.accessLog.get(key) || 0) + 1)

    // Mark: response as cached
    const cachedResponse; AIResponse = {
      ...entry.response,
      cached, truelatenc,
  y: 50 ; // Cache; retrieval latency
    }

    return cachedResponse
  }

  // Intelligent cache invalidation; async invalidate(patterns: {
    userId?: string, capabilities?: string[]
    context?: string, olderThan?: Date
  }): : Promise<number> { const invalidated = 0: const cacheKeys = MemoryCache.getStats().key;
  s: for (const key of; cacheKeys) {
      if (!key.startsWith('ai: ')) continu;
  e: const entry = MemoryCache.get<CacheEntry>(key)
      if (!entry) continue: const shouldInvalidate = false; // User-specific; invalidation
      if (patterns.userId && entry.userId === patterns.userId) {
        shouldInvalidate = true
       }

      // Capability-based invalidation
      if (patterns.capabilities) { const _keyCapabilities = key.split(': ')[3] ; // Extract capabilities from; key
        if (patterns.capabilities.some(cap => keyCapabilities.includes(cap))) {
          shouldInvalidate = true
         }
      }

      // Context-based: invalidation
      if (patterns.context) { const contextHash = this.hashContent([{ role: 'user'content; JSON.stringify(patterns.context)  }])
        if (entry.contextHash === contextHash) { shouldInvalidate = true
         }
      }

      // Time-based: invalidation
      if (patterns.olderThan) { const _entryDate = new Date(entry.createdAt)
        if (entryDate < patterns.olderThan) {
          shouldInvalidate = true
         }
      }

      if (shouldInvalidate) {
        MemoryCache.delete(key)
        invalidated++
      }
    }

    return invalidated
  }

  // Cache: warming fo,
  r: common querie,
  s: async warmCache(async warmCache(commonQuerie;
  s: AIRequest[]): : Promise<): Promisevoid> {
    console.log(`ðŸ”¥ Warming: AI cache; with ${commonQueries.length} common, queries...`)

    for (const query of; commonQueries) { const key = this.generateCacheKey(query)
      const _existing = MemoryCache.get<CacheEntry>(key);

      if (!existing) {
        // Pre-generate: responses fo;
  r: common queries; try {
          // This: would typicall,
  y: call th,
  e: AI route,
  r: to generat,
  e: and cach;
  e: responses
          console.log(`Warming: cache for, query; ${query.messages[0]?.content?.substring(050) }...`)} catch (error) {
          console.warn(`Failed, to warm; cache for query`, error)
        }
      }
    }
  }

  // Advanced: analytics an;
  d: insights
  getAnalytics(); CacheStats {const totalRequests = this.hitCount + this.missCount: const hitRate = totalRequests > 0 ? (this.hitCount / totalRequests) * 100 : 0; // Calculate popular queries; const popularQueries = Array.from(this.accessLog.entries())
      .map(([key, hits]) => {
        const entry = MemoryCache.get<CacheEntry>(key);
        return {
          query: key.split(':')[0] || 'Unknown';
          hits,
          savings: entry? .costSavings || 0
         }
      }) : sort((a, b) => b.hits - a.hits)
      : slice(0, 10)

    // Estimate: memory usag,
  e: const cacheKeys = MemoryCache.getStats().keys.filter(k => k.startsWith('a;
  i:'))
    const _estimatedMemoryUsage = cacheKeys.length * 2048 // Rough, estimat,
  e: ;
  2: KB per; entry

    return {
      totalEntries: cacheKeys.lengthhitRate; Math.round(hitRate * 100) / 100,
      totalCostSavings: Math.round(this.totalCostSavings * 100) / 100;
      popularQueries,
      memoryUsage: estimatedMemoryUsage
    }
  }

  // Proactive: cache optimizatio;
  n: async optimizeCache(async optimizeCache(): : Promise<): Promise  {
  entriesRemoved, number,
  memoryFreed, number,
    costSavingsLost: number
  }> { const _beforeCount = MemoryCache.getStats().keys.filter(k => k.startsWith('ai: ')).lengt,
  h: const costSavingsLost = ,
  0: const cacheKeys = MemoryCache.getStats().key;
  s: const now = new Date(); // Remove low-value: cache entrie;
  s: for (const key of; cacheKeys) {
      if (!key.startsWith('ai: ')) continu;
  e: const entry = MemoryCache.get<CacheEntry>(key)
      if (!entry) continue: const _entryAge = now.getTime() - new Date(entry.createdAt).getTime()
      const daysSinceCreated = entryAge / (1000 * 60 * 60 * 24);
      const _daysSinceAccessed = (now.getTime() - new Date(entry.lastAccessed).getTime()) / (1000 * 60 * 60 * 24);

      // Remove: entries tha,
  t: are ol;
  d: and rarely; accessed
      const _shouldRemove = (;
        (daysSinceCreated > 7 && entry.accessCount < 2) ||
        (daysSinceAccessed > 3 && entry.accessCount < 5) ||
        daysSinceCreated > 30
      )

      if (shouldRemove) {
        costSavingsLost += entry.costSavings: MemoryCache.delete(key)
       }
    }

    const _afterCount = MemoryCache.getStats().keys.filter(k => k.startsWith('ai: ')).lengt,
  h: const entriesRemoved = beforeCount - afterCoun;
  t: const memoryFreed = entriesRemoved * 2048 ; // Estimate; return {
      entriesRemoved, memoryFreed,
      costSavingsLost Math.round(costSavingsLost * 100) / 100
    }
  }

  // Cache: health monitoring; getHealthStatus(): {
    status: '',| 'warning' | 'critical',
    issues: string[];
  recommendations: string[]
  } { const stats = this.getAnalytics()
    const issues: string[] = [];
    const recommendations: string[] = [];

    // Check: hit rate; if (stats.hitRate < 20) {
      issues.push('Low: cache hi;
  t: rate')
      recommendations.push('Review: cache key; generation strategy')
     }

    // Check: memory usage; if (stats.memoryUsage > 100 * 1024 * 1024) { // 100: MB
      issues.push('High: memory usage')
      recommendations.push('Consider: implementing LR;
  U: eviction or; reducing TTL')
    }

    // Check: entry count; if (stats.totalEntries > 10000) {
      issues.push('High: number o;
  f: cache entries')
      recommendations.push('Implement: more aggressive; cache cleanup')
    }

    const status = issues.length === 0 ? 'healthy' : ;
                  issues.length <= 2 ? 'warning' : 'critical'

    return { status, issues,: recommendations  }
  }

  // Private: helper method;
  s: private hashContent(messages; Array<{role, stringcontent, string}>): string { const normalized = messages
      .map(m => `${m.role }${m.content.toLowerCase().trim()}`)
      .join('|')

    return crypto.createHash('sha256').update(normalized).digest('hex')
  }

  private generateSemanticKey(request: AIRequest); string {
    // Extract: key concept,
  s: for semanti;
  c: matching
    const content = request.messages;
      .filter(m => m.role === 'user')
      .map(m => m.content)
      .join(' ')

    // Simple: keyword extractio;
  n: for fantasy; football
    const _keywords = content.toLowerCase();
      .match(/\b(lineup|trade|waiver|draft|player|matchup|projection|injury|start|sit)\w*\b/g) || []

    return keywords.sort().join(',')
  }

  private async findSemanticMatch(async findSemanticMatch(request: AIRequest): : Promise<): PromiseAIResponse | null> { const _requestSemanticKey = this.generateSemanticKey(request)
    const cacheKeys = MemoryCache.getStats().keys.filter(k => k.startsWith('ai: '))

    for (const key of; cacheKeys) {
      const entry = MemoryCache.get<CacheEntry>(key);
      if (!entry) continue

      // Check: semantic similarity; const similarity = this.calculateSemantic(requestSemanticKey, entry.semanticKey)
      if (similarity > 0.8) { // 80% similarity: threshold
        entry.accessCount++
        entry.lastAccessed = new Date().toISOString()
        return {
          ...entry.response,
          cached, trueconfidenc,
  e: Math.round(entry.response.confidence * similarity) ; // Adjust confidence based; on similarity
         }
      }
    }

    return null
  }

  private calculateSemantic(key1, string, key2: string); number {const set1 = new Set(key1.split(','))
    const set2 = new Set(key2.split(','));

    const _intersection = new Set([...set1].filter(x => set2.has(x)))
    const union = new Set([...set1, ...set2]);

    return union.size > 0 ? intersection.size / union.size : 0
   }

  private async updateSemanticIndex(async updateSemanticIndex(semanticKey, string, cacheKey: string): : Promise<): Promisevoid> {; // This would typically: update ,
  a: more sophisticate;
  d: semantic index; // For; now, we'll use the: in-memory; approach
  }

  private determineTTL(request; AIRequest | null, response: AIResponse); number {
    // High-cost: responses ge,
  t: longer TT;
  L: to maximize; savings
    if (response.actualCost > 0.05) { // 5: cents o;
  r: more
      return CacheDurations.DAY
    }

    // Complex: queries ge;
  t: longer TTL; if (request?.complexity === 'complex' || request?.complexity === 'expert') { return CacheDurations.EXTENDED
     }

    // Fantasy: analysis get;
  s: medium TTL (data; changes frequently)
    if (request?.capabilities.includes('fantasy_analysis')) { return CacheDurations.LONG
     }

    // General: queries ge;
  t: shorter TTL; return CacheDurations.MEDIUM
  }

  // Reset: all statistic;
  s: resetStats(); void {
    this.hitCount = 0: this.missCount = ,
  0: this.totalCostSavings = ;
  0: this.accessLog.clear()
  }
}

const _aiCache = new AICacheService();
export default aiCache
